{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1f03dd",
   "metadata": {},
   "source": [
    "## Feedforward ANNs\n",
    "\n",
    "Input is one side and the output is another. there is recurrence\n",
    "\n",
    "Repeating the single neuron\n",
    "\n",
    "Same ips can be fed to multiple different neurons calculating something different\n",
    "Neurons in one layer can acts as inps to another layer\n",
    "\n",
    "Multiple neurons per layers\n",
    "- call output of jth nueorn \n",
    "\n",
    "Feature Hierarchies\n",
    "\n",
    "- Each layer learns increasingly complex features\n",
    " \n",
    " Neural networks breaks down a problem into smaller sub problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f066c",
   "metadata": {},
   "source": [
    "Large weights = important feature\n",
    "Small weights - Not important feature\n",
    "\n",
    "Linear neuron model not very expressive\n",
    "\n",
    "TO make the fitting line complicated - can add more input dimensions\n",
    "Make patter nonlinear\n",
    "\n",
    "Neuron model is the expression for line/plane\n",
    "\n",
    "Each neuron computes different nonlinear feature of input\n",
    "Nonlinear due to the use of sigmoid function\n",
    "\n",
    "Automatic Feature Engineering\n",
    "\n",
    "Ws and Bs are randomly initialised found iteratively using gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf49328",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "Sigmoid - 0 to 1 values.mimics biological neuron. makes neural network's decision boundary non linear\n",
    "\n",
    "Standardisation - inputs centered around 0 and approx the same range. Sigmoid op goes between 0 and 1 center being 0.5\n",
    "\n",
    "## Hyperbolic tangent\n",
    "\n",
    "Center is 0 rather than 0.5\n",
    "\n",
    "Vanishing Gradient problem - Depper the neural network, more terms have to be multiplied in. gradient due to chain rule of calculus\n",
    "\n",
    "Derivative of sigmoid very tiny. max val is 0.25\n",
    "\n",
    "Greedy layer wise pretaining is an old school training solution\n",
    "\n",
    "Solution - Use Relu (Rectified Linear unit)\n",
    "\n",
    "Relu advancements such as Leaky Relu - Slope of -1\n",
    "\n",
    "Softplus - Both softplus and ELU have vanishing gradients onthe left. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
